<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Responsible Data Science Lab at Purdue</title> <meta name="author" content="Responsible Data Science Lab at Purdue"> <meta name="description" content="Responsible Data Science Lab at Purdue "> <meta name="keywords" content="responsible data science, explainability, ML debugging"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/purdue-favicon.ico?fcd78475832b1fe1eccf62e21a8c39ad"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://romilapradhan.github.io/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">group</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/romila/">Dr. Romila Pradhan</a> <a class="dropdown-item" href="/group/">Members</a> </div> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">research</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/trustedDS/">Trusted Data Science Project</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/teaching/">CNIT 581-RDM</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Responsible Data Science Lab</span> at Purdue </h1> <p class="desc"><a href="#"></a></p> </header> <article> <div class="clearfix"> <p>We study problems at the intersection of data management and machine learning to build trustworthy and responsible decision-making systems. Our aim is to develop systems that enable explainability, fairness, and accountability of data-driven decision-making systems. We are particularly interested in:</p> <ul> <li> <b>Explaining and debugging fairness violations in machine learning models and data science pipelines:</b> <ul> <li>How can we determine sources of unexpected errors and bias in machine learning model outcomes?</li> <li>How can we decompose unexpected or discriminatory behavior of data science pipelines in terms of the different pipeline stages?</li> <li>Can we effectively generate post hoc explanations for the outcomes of machine learning models?</li> </ul> </li> <li> <b>Data integration and data quality:</b> <ul> <li>How can we leverage expert feedback to improve data cleaning techniques for machine learning?</li> <li>Can we use the final outcomes in data science pipelines to inform intermediate pipeline choices?</li> <li>How can we intertwine pipeline stages with downstream analytics to improve upon the end goals? </li> </ul> </li> </ul> <p><b> <u>We are always looking for motivated Ph.D. students to collaborate with.</u></b> If you are interested in data management and/or responsible data analytics, feel free to <a href="mailto:rpradhan@purdue.edu">contact us</a> with your CV/resume and a couple of sentences describing your research interests, and consider <a href="https://www.purdue.edu/gradschool/admissions/how-to-apply/index.html" rel="external nofollow noopener" target="_blank">applying</a> to <a href="https://www.purdue.edu/academics/ogsps/prospective/gradrequirements/westlafayette/cnit.html" rel="external nofollow noopener" target="_blank">Purdue CIT</a>!</p> <p><b> Sponsors </b> We are thankful for the generous funding award and gift from our sponsors: <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2237149" rel="external nofollow noopener" target="_blank">NSF</a>, <a href="https://research.google/outreach/research-scholar-program/recipients/?category=2022" rel="external nofollow noopener" target="_blank">Google</a>, and <a href="https://casmi.northwestern.edu/research/projects/data-biases.html" rel="external nofollow noopener" target="_blank">CASMI</a>.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Feb 4, 2026</th> <td> Our full paper on Selective Data Expansion for Model Performance got accepted to the <a href="https://edbticdt2026.github.io/" rel="external nofollow noopener" target="_blank">29th International Conference on Extending Database Technology (EDBT)</a>. Congratulations, Jahid! </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 12, 2026</th> <td> We welcome undergraduate students, Ritwik Jayaraman (as part of <a href="https://www.purdue.edu/discoverypark/duri/projects/index.php" rel="external nofollow noopener" target="_blank">DUIRI</a>) and Burton Lu, to our research group. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 12, 2026</th> <td> Two lightning talks got accepted to the <a href="https://icde2026.github.io/" rel="external nofollow noopener" target="_blank">42nd IEEE International Conference on Data Engineering (ICDE) 2026</a>. Congrats, Ambarish, Jahid, and Jingya! </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 19, 2025</th> <td> Our demonstration paper ‚ÄúSentinel: Evaluating Pipeline Robustness to Distributional Shifts‚Äù got accepted to the <a href="https://icde2026.github.io/" rel="external nofollow noopener" target="_blank">42nd IEEE International Conference on Data Engineering (ICDE) 2026</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 5, 2025</th> <td> Dr. Pradhan attended the <a href="https://vldb.org/2025/" rel="external nofollow noopener" target="_blank">VLDB conference</a> in London, UK (and also presented Shashank‚Äôs paper). </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="10.1145/3514221.3522564" class="col-sm-8"> <div class="title">Explainable AI: Foundations, Applications, Opportunities for Data Management Research</div> <div class="author"> Romila Pradhan,¬†Aditya Lahiri,¬†Sainyam Galhotra,¬†and¬†Babak Salimi</div> <div class="periodical"> <em>In Proceedings of the 2022 International Conference on Management of Data</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/xai-sigmod.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://explainable-ai-tutorial.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Algorithmic decision-making systems are successfully being adopted in a wide range of domains for diverse tasks. While the potential benefits of algorithmic decision-making are many, the importance of trusting these systems has only recently attracted attention. There is growing concern that these systems are complex, opaque and non-intuitive, and hence are difficult to trust. There has been a recent resurgence of interest in explainable artificial intelligence (XAI) that aims to reduce the opacity of a model by explaining its behavior, its predictions or both, thus allowing humans to scrutinize and trust the model. A host of technical advances have been made and several explanation methods have been proposed in recent years that address the problem of model explainability and transparency. In this tutorial, we will present these novel explanation approaches, characterize their strengths and limitations, position existing work with respect to the database (DB) community, and enumerate opportunities for data management research in the context of XAI.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="pradhan2022gopher" class="col-sm-8"> <div class="title">Interpretable Data-Based Explanations for Fairness Debugging</div> <div class="author"> Romila Pradhan,¬†Jiongli Zhu,¬†Boris Glavic,¬†and¬†Babak Salimi</div> <div class="periodical"> <em>In Proceedings of the 2021 International Conference on Management of Data</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/gopher-sigmod.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://gopher-sys.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>A wide variety of fairness metrics and eXplainable Artificial Intelligence (XAI) approaches have been proposed in the literature to identify bias in machine learning models that are used in critical real-life contexts. However, merely reporting on a model‚Äôs bias or generating explanations using existing XAI techniques is insufficient to locate and eventually mitigate sources of bias. We introduce Gopher, a system that produces compact, interpretable, and causal explanations for bias or unexpected model behavior by identifying coherent subsets of the training data that are root-causes for this behavior. Specifically, we introduce the concept ofcausal responsibility that quantifies the extent to which intervening on training data by removing or updating subsets of it can resolve the bias. Building on this concept, we develop an efficient approach for generating the top-ùëò patterns that explain model bias by utilizing techniques from the machine learning (ML) community to approximate causal responsibility, and using pruning rules to manage the large search space for patterns. Our experimental evaluation demonstrates the effectiveness of Gopher in generating interpretable explanations for identifying and debugging sources of bias.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="galhotra2021explaining" class="col-sm-8"> <div class="title">Explaining Black-Box Algorithms using Probabilistic Contrastive Counterfactuals</div> <div class="author"> Sainyam Galhotra,¬†Romila Pradhan,¬†and¬†Babak Salimi</div> <div class="periodical"> <em>In Proceedings of the 2021 International Conference on Management of Data</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/lewis-sigmod.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://lewis-system.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>There has been a recent resurgence of interest in explainable arti!cial intelligence (XAI) that aims to reduce the opaqueness of AI-based decision-making systems, allowing humans to scrutinize and trust them. Prior work in this context has focused on the attribution of responsibility for an algorithm‚Äôs decisions to its inputs wherein responsibility is typically approached as a purely associational concept. In this paper, we propose a principled causalitybased approach for explaining black-box decision-making systems that addresses limitations of existing methods in XAI. At the core of our framework lies probabilistic contrastive counterfactuals, a concept that can be traced back to philosophical, cognitive, and social foundations of theories on how humans generate and select explanations. We show how such counterfactuals can quantify the direct and indirect in!uences of a variable on decisions made by an algorithm, and provide actionable recourse for individuals negatively affected by the algorithm‚Äôs decision. Unlike prior work, our system, Lewis: (1) can compute provably effective explanations and recourse at local, global and contextual levels; (2) is designed to work with users with varying levels of background knowledge of the underlying causal model; and (3) makes no assumptions about the internals of an algorithmic system except for the availability of its input-output data. We empirically evaluate Lewis on four realworld datasets and show that it generates human-understandable explanations that improve upon state-of-the-art approaches in XAI, including the popular LIME and SHAP. Experiments on synthetic data further demonstrate the correctness of Lewis‚Äôs explanations and the scalability of its recourse algorithm.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="10.1145/3035918.3035941" class="col-sm-8"> <div class="title">Staging User Feedback toward Rapid Conflict Resolution in Data Fusion</div> <div class="author"> Romila Pradhan,¬†Siarhei Bykau,¬†and¬†Sunil Prabhakar</div> <div class="periodical"> <em>In Proceedings of the 2017 ACM International Conference on Management of Data</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/user-feedback-fusion.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In domains such as the Web, sensor networks and social media, sources often provide conflicting information for the same data item. Several data fusion techniques have been proposed recently to resolve conflicts and identify correct data. The performance of these fusion systems, while quite accurate, is far from perfect. In this paper, we propose to leverage user feedback for validating data conflicts and rapidly improving the performance of fusion. To present the most beneficial data items for the user to validate, we take advantage of the level of consensus among sources, and the output of fusion to generate an effective ordering of items. We first evaluate data items individually, and then define a novel decision-theoretic framework based on the concept of value of perfect information (VPI) to order items by their ability to boost the performance of fusion. We further derive approximate formulae to scale up the decision-theoretic framework to large-scale data. We empirically evaluate our algorithms on three real-world datasets with different characteristics, and show that the accuracy of fusion can be significantly improved even while requesting feedback on a few data items. We also show that the performance of the proposed methods depends on the characteristics of data, and assess the trade-off between the amount of feedback acquired, and the effectiveness and efficiency of the methods.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%72%70%72%61%64%68%61%6E@%70%75%72%64%75%65.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=lC4axvoAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2026 Responsible Data Science Lab at Purdue. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>